Attention
=========

Whereas attention-based models, such as transformers, have gained significant attention for natural language processing (NLP) and image processing, their potential for implementation in complex-valued problems such as signal processing remains relatively untapped. 
Here, we include complex-valued variants of several attention-based techniques. 

.. automodule:: complextorch.nn.modules.attention
    :members:

.. toctree::
    :maxdepth: 2

    attention/eca
    attention/mca